% PP-Article.tex for AEA last revised 22 June 2011
% When using the LaTeX or SW templates, please select the "finalmode" class option.
\documentclass[dvipsnames,finalmode,PP]{AEA}

%PAGE LIMITS FOR PAPERS:

%IMPORTANT NOTE: Page limits are firm. Because of space constraints, exceptions will NOT be made. Papers that do not adhere to the guidelines will be returned to the author. If, after being returned, the paper is not edited to conform to the guidelines, the paper will NOT be published.

%4-paper session:
%5 pages; (i.e., 5 pages if using our LaTeX/SW templates, 7 pages if using our Word template (if you are not using our templates: 10 double-spaced pages)), including all text, figures, tables, footnotes, acknowledgements, and references.

%%%%%% NOTE FROM OVERLEAF: The mathtime package is no longer publicly available nor distributed. We recommend using a different font package e.g. mathptmx if you'd like to use a Times font.
\usepackage{mathptmx}
\usepackage{comment}
\usepackage[yyyymmdd,hhmmss]{datetime}
\usepackage{acronym}
\input{acrodefs.tex}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\usepackage{caption}
\usepackage{float}

\DeclareMathOperator{\datax}{\mathcal{V}}
% Toggle FINAL is set within TEMPLATE.tex
\input{template.tex}
% If you have trouble with the mathtime package please see our technical support
% document at: http://www.aeaweb.org/templates/technical_support.pdf
% You may remove the mathtime package if you can't get it working but your page
% count may be inaccurate as a result.
% \usepackage[cmbold]{mathtime}

% Note: you may use either harvard or natbib (but not both) to provide a wider
% variety of citation commands than latex supports natively. See below.
\iftoggle{final}{
\newcommand{\extra}{}
}
{
\usepackage{draftwatermark}
\usepackage{transparent}
\SetWatermarkText{\color{red} \transparent{0.2}Draft. Please do not cite. }
\SetWatermarkScale{0.25}
\SetWatermarkLightness{0.9}
\newcommand{\extra}{{\color{red}This version: \today at \currenttime .}}

}



% Uncomment the next line to use the natbib package with bibtex
\usepackage{natbib}
\input{title-info.tex}

% Uncomment the next line to use the harvard package with bibtex
%\usepackage[abbr]{harvard}

% This command determines the leading (vertical space between lines) in draft mode
% with 1.5 corresponding to "double" spacing.
\draftSpacing{1.5}

\begin{document}

\title{\mytitle}
\author{\myauthors}

\shortTitle{Economics, Privacy, and Computer Science}
\pubMonth{May}
\date{January 2019}
\pubYear{2019}
\pubVolume{}
\pubIssue{}
\maketitle

%\iftoggle{final}{}{\textbf{\textcolor{red}{\tiny Toggle final is set to FALSE. See template.tex}}}



Privacy protection and scientific output are public goods. When Google displays search content clearly derivative of your recent online history or when the U.S. Census Bureau publishes geographically detailed demographic data clearly descriptive of your own neighborhood, some privacy is lost for everybody while supplying information that can be repeatedly re-used to increase utility.
%Until recently, economists have let computer scientists worry about the privacy loss while they focused on explaining the private incentives to cooperate with Internet information giants.

Economists studying privacy have not focused on decisions about privacy loss inherent in the data publication process. These issues have recently been advanced almost exclusively by computer scientists who focus on technologies for increasing information quality while protecting privacy.
%
\citet{abowdschmutte.aer.2018} showed that decisions about protecting privacy and making information public inherent in publishing data from confidential sources can be addressed using traditional social welfare analysis.
This embeds the computer scientists' contributions into a framework that allows social scientists to contribute to the debate about safe methods for analyzing and publishing confidential data.

Economists rely heavily on designed data and administrative records from governmental agencies to do critical research.
These studies are often done under the supervision of a statistical agency exercising its dual mandate to disseminate
information and to protect the privacy and confidentiality of respondent data. We have long recognized that there is
tension between these mandates. Cryptographers established in the early 2000s that there is a hard limit to the amount
of fully accurate information that can be published from any finite confidential database \citep{Dinur:2003:RIW:773153.773173}---a
budget constraint stated in terms of confidential information leakage. New methods of
confidentiality protection, known as formal privacy in computer science, quickly followed.

The implications of database reconstruction for the work of statistical agencies were largely unexplored before the U.S. Census Bureau announced its research program (\ac{CSAC} Meeting, September 2016) and its decision to implement differential privacy \citep{Dwork2006a}, the leading variant of formal privacy models, for the 2020 Census of Population (\ac{CSAC} Meeting, September 2017). The Commission on Evidence-based Policymaking \citeyearpar{cep:promise:2017} also explicitly recommended that statistical agencies embrace privacy-enhancing data analysis methods.

\input{figure.tex}

These methods enforce an explicit trade-off between privacy protection and statistical accuracy, which economists will recognize as a production function. Implementation requires that the analyst acknowledge that fitting some models privately precludes fitting others unless more privacy-loss is permitted.
An explicit choice---outside the domain of computer science, but integral to economics---must be made: what is the optimal accuracy-privacy protection point for a given collection of data. The social choice is constrained by the formal privacy technology introduced by cryptographers. The preference mapping, on the other hand, must be expressed based on the uses of the published information and the attendant confidentiality risk.
Figure \ref{fig:tradeoff} illustrates a typical production function with privacy loss ($\varepsilon$) on the $x$-axis and the accuracy of the data release on the $y$-axis. Accuracy is measured relative to releasing the data with no confidentiality protections (accuracy = 1). Two different social welfare functions are illustrated. The tangent point labeled ``Data Users'' reflects the tendency of economists and other social scientists to favor accuracy over confidentiality protection. The point labeled ``Data Custodians'' reflects the tendency of data curators, often computer scientists, to favor privacy protection over accuracy. Social scientists have behaved as if they could always have maximum accuracy in every published statistic. We must now re-design many of our analysis protocols to accommodate the constraints of provably effective privacy protection.


Economists are not the only ones. Apple \citep{Apple:Learning:2017}, Google \citep{Erlingsson2014},  Microsoft \citep{Ding:Telemetry:NIPS:2017}, and many other information technology giants face the same conundrum. Because there are both technological and social preference components to the problem, ceding the debate to computer scientists focuses too much attention on the privacy mechanism and too little attention on how to do good social science under a privacy-loss constraint. By drawing the attention of economists to their role in studying this problem, this paper begins to redress this imbalance.

\section{Scientific Integrity Is the Highest Priority}

Scientific discoveries are made by examining data using appropriate statistical techniques. We call those methods \emph{inference-valid} when, under the maintained assumptions, the statistical conclusions have the probability distributions indicated by the theory. Inference-valid analyses allow the findings to generalize beyond the data from which they were derived. Scientists prefer to use the original, unmodified data as inputs, since any modifications may compromise the validity of the inference. However, when using the original data entails the risk of a breach of confidentiality, \ac{SDL} is usually applied.

The value of \ac{SDL} should not be measured merely as a function of its ability to protect against privacy loss, though this is surely important. Its value also lies in its ability to provide data that admit inference-valid analysis. Traditional \ac{SDL} methods fail to uphold this principle \citep{abowd:schmutte:BPEA:2015}.
%But inference-validity can and should be fully embodied in a modern \ac{SDL} system based on formal privacy principles.
But inference-validity should be fully embodied in a modern \ac{SDL} system, and formal privacy principles make this possible.

\section{The Roles to be Played by Economists}

Amid the sea change in the way confidential data are made available for research, economists have two roles to play. As data users, we must gain a clearer understanding of what these changes mean for our ability to conduct valid research. The policy decisions made at statistical agencies have the potential to improve or further compromise inferential validity on any research question. Economists must be at the table as these decisions are made.

At a more fundamental level, economists can help guide policy-makers in deciding how to trade data accuracy off against privacy protection. The database reconstruction theorem implies that the information in a confidential database is finite. It can be allocated between the competing uses of protecting privacy or publishing more accurate statistics. This problem is in the economist's wheelhouse, particularly given that both uses are public goods.

\cite{abowdschmutte.aer.2018} describe this basic public choice problem, highlighting the key open areas for research. Fundamentally, we need to understand the social value of accessible, accurate data, and the social value of protecting the underlying confidential micro-data. Social scientists typically behave as if the social benefits of high-quality widely available data massively exceed the social costs of any associated privacy loss. This belief is not based on any rigorous theoretical or empirical evidence that we have found.\footnote{The literature on the value of public data is remarkably thin, notwithstanding early and important contribution of \cite{Spencer:Optimal:JASA:1985}, who developed a framework for modeling optimal data quality, and \cite{Sims1985}, who argued against the logical consistency of standard cost-benefit analysis for public data.}
By contrast, cryptographers and other privacy experts tend to behave as if the social costs of privacy loss dwarf the benefits of data quality. To date, there are some models of the private demand for privacy \citep{Ghosh:Auction:GEB:2015, Nissim:2012:PMD:2229012.2229073}, as well as a growing evidence base for the private costs of privacy loss \citep[e.g.][]{Acquisti2013}.

\section{Traditional SDL Is Broken}

Some resistance to the modernization of privacy protection arises from the mistaken belief that traditional \ac{SDL} necessarily produces more reliable or even exact data with trivial re-identification risks \citep{ruggles:etal:2018}. Newer methods are unfamiliar, while there are decades of research using data produced with traditional \ac{SDL}. Researchers must replace general understanding of formal privacy with correctly reasoned comparisons of feasible alternatives.

It is important to realize that traditional \ac{SDL} presents significant problems for social scientific research. Furthermore, the data demands imposed by quasi-experimental research designs exacerbate these flaws. The secrecy surrounding traditional \ac{SDL} is a fatal flaw for social science. For example, when publishing micro-data, statistical agencies commonly swap records. The swap rate, the algorithm used to determine whether a record is at risk for swapping, and how the swapping is actually implemented, are all kept secret because there is no formal model to demonstrate that ``enough'' swapping was done. It might then be possible to undo the confidentiality protection afforded by the swapping \citep{abowd:schmutte:BPEA:2015}.

Aside from the possible biases that swapping and other methods may introduce, traditional \ac{SDL} introduces variability into the published data that should affect our inferences about what the underlying confidential data say about the world. This source of variability is almost never explicitly addressed in ensuring that inferences based on \ac{SDL}-protected data are valid. Even if we wanted to, because the details of traditional \ac{SDL} are kept secret, it is usually not possible to account for it in estimation and inference.

Traditional \ac{SDL} can also lead to bias in common research designs. \cite{abowd:schmutte:BPEA:2015} show that current \ac{SDL} practices introduce bias into estimates from linear regression models, instrumental variable models, and regression discontinuity studies. Analyses based on tabulated data, like the \ac{QCEW}, are compromised by \ac{SDL} rules that require cells influenced by just a few observations to be suppressed. The suppression rules are generally vague, and in most studies, this suppression is nonignorable. Researchers have become comfortable with the practice of performing the analysis on the available data using the implicit assumption that suppressed data are missing at random. We should aspire to do better. We should aspire to procedures that are provably inference valid.

\begin{comment}
Also give the detailed example of decoarsening the PUMA in the ACS PUMS using the 5-year tables.

WNS: If memory serves, we tabled the decoarsening of PUMAs work around the time I started at Census given the mismatch of pre/post 2012 PUMAs in the 5-year synthetic U.S. data. Of course you might be referring to a different example than what I'm thinking of.
\end{comment}

\section{Formal Privacy Takes, but also Gives}

A major concern regarding formal privacy systems is that they will change the ways in which researchers can access data, particularly micro-data. Exactly how formal privacy systems will affect the publication of detailed micro-data is the subject of extensive current research. Any change to the way published micro-data are distorted is a matter of form and degree.

It is natural to mourn the loss of familiar data summaries, particularly as they may cause a break in continuity of data releases.
%\footnote{Again, on this point, economists and other social scientists can contribute by arguing aggressively, with theory and evidence, for the value of continuing long-running data series.} However, modern disclosure avoidance systems also allow for publication of data that would be unthinkable under traditional \ac{SDL}.
But formal privacy methods also allow  publishing new tabulations with far more detail than traditionally possible.
Using input noise infusion, the Census Bureau publishes the \ac{QWI},  county-industry level data on employment and job flows with demographic details and minimal suppression \citep{AbowdEtAl2009}.
In the first official statistical publication using differential privacy, the Census Bureau  publishes \ac{LODES}, complete block-level data on commuting flows \citep{Machanavajjhala:OTM:ICDE:2008}. The \ac{PSEO} pilot release \citep{USCensusBureau2018} relies on differential privacy to publish detailed earnings and employment outcomes for college and university graduates by degree level. Most recently, a team of Census Bureau and academics published the {Opportunity Atlas} \citep{Chetty2018}, which provides inference-valid tract-level summaries of inter-generational mobility by race and gender---an outcome that is not feasible using traditional \ac{SDL}.

\section{Computer Scientists Are Right about Re-identification}

%There is no point in continuing to argue about this.
The cryptographers found a fundamental defect in the approach statistical agencies have historically taken to \ac{SDL}. The database reconstruction theorem shows that it is always possible to reconstruct part or all of a confidential database using combinations of statistics published from that database. Therefore, even the publication of tabular summaries from, say, the decennial census or the American Community Survey is tantamount to a data security breach that releases all or part of the confidential database. Every variable in the reconstructed micro-data is a potential identifier, even if the name and exact address cannot be reconstructed. Putting aside the legal and ethical questions of what constitutes a meaningful breach of privacy, it is fair to say that if we woke up tomorrow and learned that 50 percent of decennial census records, including detailed geography, had been exposed, we would find the statistical system under attack whether or not individuals could be re-identified from those released data.

Differential privacy does not provide absolute protection against the disclosure of sensitive information. It trades absolute claims for relative ones, acknowledging at its core the impossibility of providing useful data summaries and complete privacy protection \citep{Dwork2006a}. Formal methods control the global risk from reconstruction-abetted re-identification attacks using the privacy-loss budget $\epsilon$. An adversary with auxiliary information that includes traditional identifiers (e.g., name and address) along with information that matches variables released via differential privacy, cannot improve the accuracy of any linkage for any person or any variable by more than a multiplicative factor of $e^{2\epsilon}$ (see the Online Appendix for details). If a statistical agency wants to provably limit linkage-based re-identification attacks with a public degree of confidence, then it has no currently feasible choice except to adopt formal methods and stand by the privacy-loss budget it sets.

Traditional \ac{SDL} also relies on uncertainty about whether a linkage-based attack produces a reliable re-identification. But agencies do not discuss the quantification of this risk---they do not release statistics on putative re-identifications (the number of records in the confidential database that their internal experiments were able to re-identify) nor on confirmed re-identifications (the number of putative re-identifications that were correct). If they did, one could discuss whether such a confirmation rate is acceptable. If a particular confirmation rate for re-identifications is acceptable, then formal methods can insure that the released data are consistent with a stated level of uncertainty about correct linkage re-identifications. For example, $\epsilon = 1.0$ guarantees that the improvement in the odds of a successful re-identification never exceeds $7.4:1$ for \emph{any person} in the population when that person's data are used in the publications versus when they are deleted or replaced with an arbitrary record. An $\epsilon =0.25$ guarantees that the improvement in the odds never exceeds $1.65:1$, and an $\epsilon=0.1$ guarantees that the improvement never exceeds $1.2:1$. Many more examples of differential privacy's provable protection against re-identification can be found in \citet{wood:etal:JET:2018}.

\section{Moving Forward}

To make progress, we should agree on the principles used to evaluate confidentiality protection mechanisms, whether traditional or formally private. Three components are essential.

First, agree on a \emph{replication protocol} that confirms the provenance and authenticity of public-use inputs such as particular public-use data releases. Next, it identifies and confirms the provenance of the computations applied to those inputs to generate a specific set of outputs. Finally, the replication protocol confirms applying these computations to the public-use inputs produces the published outputs claimed in a particular scientific paper.

Second, agree on a \emph{validation protocol} that confirms the provenance and authenticity of the confidential inputs used to produce the versions of the public-use inputs in the replication protocol. Next, it certifies the mapping from the computations applied in the replication protocol to the computations that must be applied to the confidential inputs to perform the same statistical analysis. Finally, the validation protocol produces outputs that are directly comparable to the outputs from the replication protocol.

Third, agree on a \emph{comparison protocol}. Multiple candidate and historical public-use products may be put through the replication and validation protocols.  The comparison protocol specifies how the validations will be compared, given that the replications are correct. Only the validations should be compared, because these establish the properties of the scientific inferences, given the confidential data. There is no point in directly comparing replications from alternative inputs because such comparisons have no standard for correctness.

Ideally, an independent panel would conduct this process. However, such a panel would have difficulty vetting the validation protocol because curating the definitive versions of the confidential inputs to particular public-use products is very resource intensive. The Census Bureau's synthetic data program for the \ac{SIPP} illustrates the commitment associated with maintaining replication and validation protocols \citep{benedetto:SIPP:v7}.

Statistical agencies must commit resources to the research program outlined here. Professional organizations and curators of research data must be prepared to work with the agencies. Going forward, cooperation in achieving the objectives outlined in this section would position both the agencies and the research community to have increased confidence in the privacy protections and the scientific validity of all analyses based on the agencies' data.



\bibliographystyle{aea}
\bibliography{library}

% The appendix command is issued once, prior to all appendices, if any.
\appendix

% \section{Mathematical Appendix}

%\section{Extra}
%\input{braindump.tex}

\section{Online Appendix}
\input{appendix.tex}

\end{document}
